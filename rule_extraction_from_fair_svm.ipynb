{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import minimize\n",
    "SEED = 1122334455\n",
    "seed(SEED) # set the random seed so that the random permutations can be reproduced again\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def print_classifier_fairness_stats(acc_arr, correlation_dict_arr, cov_dict_arr, s_attr_name):\n",
    "    correlation_dict = get_avg_correlation_dict(correlation_dict_arr)\n",
    "    #print(correlation_dict)\n",
    "    try:\n",
    "        non_prot_pos =correlation_dict[s_attr_name][1][1]\n",
    "    except:\n",
    "        non_prot_pos = 0\n",
    "    try:\n",
    "        prot_pos = correlation_dict[s_attr_name][0][1]\n",
    "    except:\n",
    "        prot_pos = 0\n",
    "\n",
    "    print (\"Accuracy: %0.2f\" % (np.mean(acc_arr)))\n",
    "    print (\"Protected/non-protected in +ve class: %0.0f%% / %0.0f%%\" % (prot_pos, non_prot_pos))\n",
    "    print (\"Covariance between sensitive feature and decision from distance boundary : %0.3f\" % (np.mean([v[s_attr_name] for v in cov_dict_arr])))\n",
    "\n",
    "def get_one_hot_encoding(in_arr):\n",
    "    \"\"\"\n",
    "        input: 1-D arr with int vals -- if not int vals, will raise an error\n",
    "        output: m (ndarray): one-hot encoded matrix\n",
    "                d (dict): also returns a dictionary original_val -> column in encoded matrix\n",
    "    \"\"\"\n",
    "\n",
    "    for k in in_arr:\n",
    "        if str(type(k)) != \"<type 'numpy.float64'>\" and type(k) != int and type(k) != np.int64:\n",
    "            print (str(type(k)))\n",
    "            print (\"************* ERROR: Input arr does not have integer types\")\n",
    "            return None\n",
    "        \n",
    "    in_arr = np.array(in_arr, dtype=int)\n",
    "    assert(len(in_arr.shape)==1) # no column, means it was a 1-D arr\n",
    "    attr_vals_uniq_sorted = sorted(list(set(in_arr)))\n",
    "    num_uniq_vals = len(attr_vals_uniq_sorted)\n",
    "    if (num_uniq_vals == 2) and (attr_vals_uniq_sorted[0] == 0 and attr_vals_uniq_sorted[1] == 1):\n",
    "        return in_arr, None\n",
    "\n",
    "    \n",
    "    index_dict = {} # value to the column number\n",
    "    for i in range(0,len(attr_vals_uniq_sorted)):\n",
    "        val = attr_vals_uniq_sorted[i]\n",
    "        index_dict[val] = i\n",
    "\n",
    "    out_arr = []    \n",
    "    for i in range(0,len(in_arr)):\n",
    "        tup = np.zeros(num_uniq_vals)\n",
    "        val = in_arr[i]\n",
    "        ind = index_dict[val]\n",
    "        tup[ind] = 1 # set that value of tuple to 1\n",
    "        out_arr.append(tup)\n",
    "\n",
    "    return np.array(out_arr), index_dict\n",
    "\n",
    "def check_accuracy(model, x_train, y_train, x_test, y_test, y_train_predicted, y_test_predicted):\n",
    "\n",
    "    print(\"weight dims\", len(model))\n",
    "    #if not model:\n",
    "    #    print (\"Invalid Model \")\n",
    "    #    assert(0)\n",
    "    \"\"\"\n",
    "    returns the train/test accuracy of the model\n",
    "    we either pass the model (w)\n",
    "    else we pass y_predicted\n",
    "    \"\"\"\n",
    "    if model is not None and y_test_predicted is not None:\n",
    "        print (\"Either the model (w) or the predicted labels should be None\")\n",
    "        raise Exception(\"Either the model (w) or the predicted labels should be None\")\n",
    "\n",
    "    if model is not None:\n",
    "        print (\"Model Predictions\")\n",
    "        y_test_predicted = np.sign(np.dot(x_test, model))\n",
    "        y_train_predicted = np.sign(np.dot(x_train, model))\n",
    "\n",
    "    def get_accuracy(y, Y_predicted):\n",
    "        correct_answers = (Y_predicted == y).astype(int) # will have 1 when the prediction and the actual label match\n",
    "        #print(\" correct : \", correct_answers, \" len : \", len(correct_answers))\n",
    "        accuracy = float(sum(correct_answers)) / float(len(correct_answers))\n",
    "        return accuracy, sum(correct_answers)\n",
    "    #print (\"Predictions \",y_train,y_train_predicted.T)\n",
    "    train_score, correct_answers_train = get_accuracy(y_train, y_train_predicted[0].T)\n",
    "    test_score, correct_answers_test = get_accuracy(y_test, y_test_predicted[0])\n",
    "    print(\"Train Done\")\n",
    "\n",
    "    return train_score, test_score, correct_answers_train, correct_answers_test\n",
    "\n",
    "def test_sensitive_attr_constraint_cov(model, x_arr, y_arr_dist_boundary, x_control, thresh, verbose):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    The covariance is computed b/w the sensitive attr val and the distance from the boundary\n",
    "    If the model is None, we assume that the y_arr_dist_boundary contains the distace from the decision boundary\n",
    "    If the model is not None, we just compute a dot product or model and x_arr\n",
    "    for the case of SVM, we pass the distace from bounday becase the intercept in internalized for the class\n",
    "    and we have compute the distance using the project function\n",
    "    this function will return -1 if the constraint specified by thresh parameter is not satifsified\n",
    "    otherwise it will reutrn +1\n",
    "    if the return value is >=0, then the constraint is satisfied\n",
    "    \"\"\"\n",
    "\n",
    "    assert(x_arr.shape[0] == x_control.shape[0])\n",
    "    if len(x_control.shape) > 1: # make sure we just have one column in the array\n",
    "        assert(x_control.shape[1] == 1)\n",
    "    \n",
    "    arr = []\n",
    "    if model is None:\n",
    "        arr = y_arr_dist_boundary # simply the output labels\n",
    "    else:\n",
    "        arr = np.dot(model, x_arr.T) # the product with the weight vector -- the sign of this is the output label\n",
    "    \n",
    "    arr = np.array(arr, dtype=np.float64)\n",
    "\n",
    "\n",
    "    cov = np.dot(x_control - np.mean(x_control), arr ) / float(len(x_control))\n",
    "\n",
    "        \n",
    "    ans = thresh - abs(cov) # will be <0 if the covariance is greater than thresh -- that is, the condition is not satisfied\n",
    "    # ans = thresh - cov # will be <0 if the covariance is greater than thresh -- that is, the condition is not satisfied\n",
    "    if verbose is True:\n",
    "        print (\"Covariance is\", cov)\n",
    "        print (\"Diff is:\", ans)\n",
    "        print\n",
    "    return ans\n",
    "\n",
    "def print_covariance_sensitive_attrs(model, x_arr, y_arr_dist_boundary, x_control, sensitive_attrs):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    reutrns the covariance between sensitive features and distance from decision boundary\n",
    "    \"\"\"\n",
    "\n",
    "    arr = []\n",
    "    if model is None:\n",
    "        arr = y_arr_dist_boundary # simplt the output labels\n",
    "    else:\n",
    "        arr = np.dot(model, x_arr.T) # the product with the weight vector -- the sign of this is the output label\n",
    "    \n",
    "\n",
    "    sensitive_attrs_to_cov_original = {}\n",
    "    for attr in sensitive_attrs:\n",
    "\n",
    "        attr_arr = x_control[attr]\n",
    "\n",
    "\n",
    "        bin_attr = check_binary(attr_arr) # check if the attribute is binary (0/1), or has more than 2 vals\n",
    "        if bin_attr == False: # if its a non-binary sensitive feature, then perform one-hot-encoding\n",
    "            attr_arr = [int(x) for x in attr_arr]\n",
    "            attr_arr_transformed, index_dict = get_one_hot_encoding(attr_arr)\n",
    "\n",
    "        thresh = 0\n",
    "\n",
    "        if bin_attr:\n",
    "            cov = thresh - test_sensitive_attr_constraint_cov(None, x_arr, arr, np.array(attr_arr), thresh, False)\n",
    "            sensitive_attrs_to_cov_original[attr] = cov\n",
    "        else: # sensitive feature has more than 2 categorical values            \n",
    "            \n",
    "            cov_arr = []\n",
    "            sensitive_attrs_to_cov_original[attr] = {}\n",
    "            for attr_val, ind in index_dict.items():\n",
    "                t = attr_arr_transformed[:,ind]\n",
    "                cov = thresh - test_sensitive_attr_constraint_cov(None, x_arr, arr, t, thresh, False)\n",
    "                sensitive_attrs_to_cov_original[attr][attr_val] = cov\n",
    "                cov_arr.append(abs(cov))\n",
    "\n",
    "            cov = max(cov_arr)\n",
    "            \n",
    "    return sensitive_attrs_to_cov_original\n",
    "\n",
    "\n",
    "def get_correlations(model, x_test, y_predicted, x_control_test, sensitive_attrs):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    returns the fraction in positive class for sensitive feature values\n",
    "    \"\"\"\n",
    "\n",
    "    if model is not None:\n",
    "        y_predicted = np.sign(np.dot(x_test, model))\n",
    "        \n",
    "    y_predicted = np.array(y_predicted)\n",
    "    \n",
    "    out_dict = {}\n",
    "    for attr in sensitive_attrs:\n",
    "\n",
    "        attr_val = []\n",
    "        for v in x_control_test[attr]: attr_val.append(v)\n",
    "        assert(len(attr_val) == len(y_predicted))\n",
    "\n",
    "\n",
    "        total_per_val = defaultdict(int)\n",
    "        attr_to_class_labels_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for i in range(0, len(y_predicted)):\n",
    "            val = attr_val[i]\n",
    "            label = y_predicted[i]\n",
    "\n",
    "            if isinstance(label, np.float64):\n",
    "                label = np.array([label])\n",
    "            # val = attr_val_int_mapping_dict_reversed[val] # change values from intgers to actual names\n",
    "            total_per_val[val] += 1\n",
    "            attr_to_class_labels_dict[val][label[0]] += 1\n",
    "\n",
    "        class_labels = y_predicted.tolist()\n",
    "\n",
    "        local_dict_1 = {}\n",
    "        for k1,v1 in attr_to_class_labels_dict.items():\n",
    "            total_this_val = total_per_val[k1]\n",
    "\n",
    "            local_dict_2 = {}\n",
    "            for k2 in class_labels: # the order should be the same for printing\n",
    "                if type(k2) is list:\n",
    "                    v2 = v1[k2[0]]\n",
    "                else:\n",
    "                    v2 = v1[k2]\n",
    "\n",
    "                f = float(v2) * 100.0 / float(total_this_val)\n",
    "\n",
    "                if type(k2) is list:\n",
    "                    local_dict_2[k2[0]] = f\n",
    "                else:\n",
    "                    local_dict_2[k2] = f\n",
    "            local_dict_1[k1] = local_dict_2\n",
    "        out_dict[attr] = local_dict_1\n",
    "\n",
    "    return out_dict\n",
    "\n",
    "def check_binary(arr):\n",
    "    \"give an array of values, see if the values are only 0 and 1\"\n",
    "    s = sorted(set(arr))\n",
    "    if s[0] == 0 and s[1] == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_avg_correlation_dict(correlation_dict_arr):\n",
    "    # make the structure for the correlation dict\n",
    "\n",
    "    correlation_dict_avg = {}\n",
    "\n",
    "    for k,v in correlation_dict_arr.items():\n",
    "        correlation_dict_avg[k] = {}\n",
    "        for feature_val, feature_dict in v.items():\n",
    "            correlation_dict_avg[k][feature_val] = {}\n",
    "\n",
    "            for class_label, frac_class in feature_dict.items():\n",
    "                correlation_dict_avg[k][feature_val][class_label] = []\n",
    "\n",
    "    # populate the correlation dict\n",
    "    if type(correlation_dict_arr) is not list:\n",
    "        correlation_dict_arr = [correlation_dict_arr]\n",
    "    for correlation_dict in correlation_dict_arr:\n",
    "\n",
    "        for k,v in correlation_dict.items():\n",
    "            for feature_val, feature_dict in v.items():\n",
    "\n",
    "                for class_label, frac_class in feature_dict.items():\n",
    "                    correlation_dict_avg[k][feature_val][class_label].append(frac_class)\n",
    "\n",
    "    # now take the averages\n",
    "    for k,v in correlation_dict_avg.items():\n",
    "        for feature_val, feature_dict in v.items():\n",
    "            for class_label, frac_class_arr in feature_dict.items():\n",
    "                correlation_dict_avg[k][feature_val][class_label] = np.mean(frac_class_arr)\n",
    "\n",
    "    return correlation_dict_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Numpy for scientific calculation\n",
    "import numpy as np\n",
    "# from helper import *\n",
    "from copy import deepcopy\n",
    "from scipy.optimize import minimize\n",
    "class SVM:\n",
    "\n",
    "    def predict(self,X,weights,lambd):\n",
    "        return np.dot(X,weights) + np.sum(lambd*(weights**2))\n",
    "\n",
    "    def findCost(self,Y,pred):\n",
    "        value=Y*pred\n",
    "        if value >1:\n",
    "            cost=0\n",
    "        else :\n",
    "            cost=1-value\n",
    "        return cost\n",
    "    \n",
    "    def training(self,x,y,x_control,loss_function,C,max_iter,lamb,epochs=500,lr=1, apply_fairness_constraints = 0, sensitive_attrs = ['sex'], sensitive_attrs_to_cov_thresh = {},gamma=None):\n",
    "        '''\n",
    "        This function return the model weight after training\n",
    "        '''\n",
    "    \n",
    "        max_iter = max_iter # maximum number of iterations for the minimization algorithm\n",
    "\n",
    "        if apply_fairness_constraints == 0:\n",
    "            w = self.traindef(x,y,lamb,C,epochs,lr)\n",
    "            return w\n",
    "        \n",
    "        elif apply_fairness_constraints == 1:\n",
    "            print (\"running Custom model\")\n",
    "\n",
    "            if gamma is not None and gamma !=0:\n",
    "\n",
    "                w = minimize(fun=loss_function,\n",
    "                             x0=np.random.rand(x.shape[1], ),\n",
    "                             args=(x, y,C),\n",
    "                             method='SLSQP',\n",
    "                             options={\"maxiter\": max_iter},\n",
    "                             constraints=[]\n",
    "                             )\n",
    "\n",
    "                old_w = deepcopy(w.x)\n",
    "\n",
    "                def constraint_gamma_all(w, x, y, C,initial_loss_arr):\n",
    "\n",
    "                    new_loss = loss_function(w, x, y,C)\n",
    "                    old_loss = initial_loss_arr\n",
    "                    return ((1.0 + gamma) * old_loss) - new_loss\n",
    "\n",
    "                def constraint_protected_people(w, x,y):  # dont confuse the protected here with the sensitive feature protected/non-protected values -- protected here means that these points should not be misclassified to negative class\n",
    "                    return np.dot(w, x.T)  # if this is positive, the constraint is satisfied\n",
    "\n",
    "                def constraint_unprotected_people(w, old_loss, x, y,C):\n",
    "\n",
    "                    new_loss = loss_function(w, np.array([x]), np.array(y),C)\n",
    "                    return ((1.0 + gamma) * old_loss) - new_loss\n",
    "\n",
    "                constraints = []\n",
    "                unconstrained_loss_arr = loss_function(w.x, x, y,C)\n",
    "                predicted_labels = np.sign(np.dot(w.x, x.T))\n",
    "                for i in range(0, len(predicted_labels)):\n",
    "                    if predicted_labels[i] == 1.0 and x_control[sensitive_attrs[0]][i] == 1.0:  # for now we are assuming just one sensitive attr for reverse constraint, later, extend the code to take into account multiple sensitive attrs\n",
    "                        c = ({'type': 'ineq', 'fun': constraint_protected_people, 'args': (x[i], y[i])})  # this constraint makes sure that these people stay in the positive class even in the modified classifier\n",
    "                        constraints.append(c)\n",
    "                    else:\n",
    "                        c = ({'type': 'ineq', 'fun': constraint_unprotected_people,\n",
    "                              'args': (unconstrained_loss_arr, x[i], y[i],C)})\n",
    "                        constraints.append(c)\n",
    "\n",
    "                def cross_cov_abs_optm_func(weight_vec, x_in, x_control_in_arr):\n",
    "                    cross_cov = (x_control_in_arr - np.mean(x_control_in_arr)) * np.dot(weight_vec, x_in.T)\n",
    "                    return float(abs(sum(cross_cov))) / float(x_in.shape[0])\n",
    "\n",
    "                w = minimize(fun=cross_cov_abs_optm_func,\n",
    "                             x0=old_w,\n",
    "                             args=(x, x_control[sensitive_attrs[0]]),\n",
    "                             method='SLSQP',\n",
    "                             options={\"maxiter\": 100000},\n",
    "                             constraints=constraints\n",
    "                             )\n",
    "\n",
    "            else:\n",
    "\n",
    "                constraints = self.get_constraint_list_cov(x, y,x_control,sensitive_attrs, sensitive_attrs_to_cov_thresh)\n",
    "                f_args = (x, y,C)\n",
    "                w = minimize(fun=loss_function,\n",
    "                             x0=np.random.rand(x.shape[1], ),\n",
    "                             args=f_args,\n",
    "                             method='SLSQP',\n",
    "                             options={\"maxiter\": max_iter},\n",
    "                             constraints=constraints\n",
    "                             )\n",
    "        return w.x\n",
    "        \n",
    "    def predict(self,x_test,w):\n",
    "        return np.sign(np.dot(np.array(x_test),w))\n",
    "        \n",
    "    \n",
    "    def get_constraint_list_cov(self, x_train, y_train, x_control_train,sensitive_attrs, sensitive_attrs_to_cov_thresh):\n",
    "\n",
    "        \"\"\"\n",
    "        get the list of constraints to be fed to the minimizer\n",
    "        \"\"\"\n",
    "\n",
    "        constraints = []\n",
    "        \n",
    "\n",
    "        for attr in sensitive_attrs:\n",
    "\n",
    "\n",
    "            attr_arr = x_control_train[attr]\n",
    "            attr_arr = [int(x) for x in attr_arr]\n",
    "            attr_arr_transformed, index_dict = get_one_hot_encoding(attr_arr)\n",
    "                \n",
    "            if index_dict is None: # binary attribute\n",
    "                thresh = sensitive_attrs_to_cov_thresh[attr]\n",
    "\n",
    "                c = ({'type': 'ineq', 'fun': test_sensitive_attr_constraint_cov, 'args':(x_train, y_train, attr_arr_transformed,thresh, False)})\n",
    "                constraints.append(c)\n",
    "            else: # otherwise, its a categorical attribute, so we need to set the cov thresh for each value separately\n",
    "\n",
    "\n",
    "                for attr_val, ind in index_dict.items():\n",
    "                    attr_name = attr_val                \n",
    "                    thresh = sensitive_attrs_to_cov_thresh[attr][attr_name]\n",
    "                \n",
    "                    t = attr_arr_transformed[:,ind]\n",
    "                    c = ({'type': 'ineq', 'fun': test_sensitive_attr_constraint_cov, 'args':(x_train, y_train,t ,thresh, False)})\n",
    "                    constraints.append(c)\n",
    "\n",
    "\n",
    "\n",
    "        return constraints\n",
    "        \n",
    "        \n",
    "    def traindef(self,X,Y,lamb,C,epochs=500,lr=1):    # Input dimensions  X:(m,n) Y:(m,1)\n",
    "        '''\n",
    "        Training SVM using Gradient decent approach\n",
    "        Parameters\n",
    "        ----------\n",
    "        X - Input features\n",
    "        Y - Labels\n",
    "        epochs - Number of epochs\n",
    "        lr - learning rate for gradient decent\n",
    "        Returns - The model weights\n",
    "        '''\n",
    "\n",
    "        print (\"Train Default model\")\n",
    "\n",
    "        n,m=X.shape[1] ,X.shape[0]\n",
    "        w= np.zeros((n, 1))\n",
    "\n",
    "        for epoch in range(1,int(epochs)):\n",
    "            for i,x in enumerate(X):\n",
    "                x_train=x.reshape(1,-1)\n",
    "                y_train = Y[i].reshape(1,1)\n",
    "                y_hat= np.dot(x_train,w) +np.sum(lamb * (w ** 2))\n",
    "                val=y_train *y_hat\n",
    "                loss = [0  if val> 1 else 1 -val]\n",
    "                if loss==0:\n",
    "                    grad=np.zeros(w.shape)\n",
    "                    w = w - lr* (grad + 2*lamb*w*C)\n",
    "                else:\n",
    "                    grad=(-y_train*x_train).T\n",
    "                    w = w - lr*(grad + 2 *lamb*w*C )\n",
    "\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##global variables\n",
    "tz=7\n",
    "tpr_by_fpr_min_thresh=0\n",
    "\n",
    "tc=400\n",
    "target_column_name='income'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"datasets/adult.csv\")\n",
    "# data=data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['income'].replace(['<=50K', '>50K'],\n",
    "                        [0, 1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_catagorical_vars=['workclass','education',\n",
    "       'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoded_to_orig_dict={}\n",
    "for col_name in list_of_catagorical_vars:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data[col_name])\n",
    "    data[col_name]=le.transform(data[col_name])\n",
    "    \n",
    "\n",
    "    encoded_to_orig_dict[col_name]= dict((v,k) for k,v in dict(zip(le.classes_, le.transform(le.classes_))).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workclass': {0: '?',\n",
       "  1: 'Federal-gov',\n",
       "  2: 'Local-gov',\n",
       "  3: 'Never-worked',\n",
       "  4: 'Private',\n",
       "  5: 'Self-emp-inc',\n",
       "  6: 'Self-emp-not-inc',\n",
       "  7: 'State-gov',\n",
       "  8: 'Without-pay'},\n",
       " 'education': {0: '10th',\n",
       "  1: '11th',\n",
       "  2: '12th',\n",
       "  3: '1st-4th',\n",
       "  4: '5th-6th',\n",
       "  5: '7th-8th',\n",
       "  6: '9th',\n",
       "  7: 'Assoc-acdm',\n",
       "  8: 'Assoc-voc',\n",
       "  9: 'Bachelors',\n",
       "  10: 'Doctorate',\n",
       "  11: 'HS-grad',\n",
       "  12: 'Masters',\n",
       "  13: 'Preschool',\n",
       "  14: 'Prof-school',\n",
       "  15: 'Some-college'},\n",
       " 'marital.status': {0: 'Divorced',\n",
       "  1: 'Married-AF-spouse',\n",
       "  2: 'Married-civ-spouse',\n",
       "  3: 'Married-spouse-absent',\n",
       "  4: 'Never-married',\n",
       "  5: 'Separated',\n",
       "  6: 'Widowed'},\n",
       " 'occupation': {0: '?',\n",
       "  1: 'Adm-clerical',\n",
       "  2: 'Armed-Forces',\n",
       "  3: 'Craft-repair',\n",
       "  4: 'Exec-managerial',\n",
       "  5: 'Farming-fishing',\n",
       "  6: 'Handlers-cleaners',\n",
       "  7: 'Machine-op-inspct',\n",
       "  8: 'Other-service',\n",
       "  9: 'Priv-house-serv',\n",
       "  10: 'Prof-specialty',\n",
       "  11: 'Protective-serv',\n",
       "  12: 'Sales',\n",
       "  13: 'Tech-support',\n",
       "  14: 'Transport-moving'},\n",
       " 'relationship': {0: 'Husband',\n",
       "  1: 'Not-in-family',\n",
       "  2: 'Other-relative',\n",
       "  3: 'Own-child',\n",
       "  4: 'Unmarried',\n",
       "  5: 'Wife'},\n",
       " 'race': {0: 'Amer-Indian-Eskimo',\n",
       "  1: 'Asian-Pac-Islander',\n",
       "  2: 'Black',\n",
       "  3: 'Other',\n",
       "  4: 'White'},\n",
       " 'sex': {0: 'Female', 1: 'Male'},\n",
       " 'native.country': {0: '?',\n",
       "  1: 'Cambodia',\n",
       "  2: 'Canada',\n",
       "  3: 'China',\n",
       "  4: 'Columbia',\n",
       "  5: 'Cuba',\n",
       "  6: 'Dominican-Republic',\n",
       "  7: 'Ecuador',\n",
       "  8: 'El-Salvador',\n",
       "  9: 'England',\n",
       "  10: 'France',\n",
       "  11: 'Germany',\n",
       "  12: 'Greece',\n",
       "  13: 'Guatemala',\n",
       "  14: 'Haiti',\n",
       "  15: 'Holand-Netherlands',\n",
       "  16: 'Honduras',\n",
       "  17: 'Hong',\n",
       "  18: 'Hungary',\n",
       "  19: 'India',\n",
       "  20: 'Iran',\n",
       "  21: 'Ireland',\n",
       "  22: 'Italy',\n",
       "  23: 'Jamaica',\n",
       "  24: 'Japan',\n",
       "  25: 'Laos',\n",
       "  26: 'Mexico',\n",
       "  27: 'Nicaragua',\n",
       "  28: 'Outlying-US(Guam-USVI-etc)',\n",
       "  29: 'Peru',\n",
       "  30: 'Philippines',\n",
       "  31: 'Poland',\n",
       "  32: 'Portugal',\n",
       "  33: 'Puerto-Rico',\n",
       "  34: 'Scotland',\n",
       "  35: 'South',\n",
       "  36: 'Taiwan',\n",
       "  37: 'Thailand',\n",
       "  38: 'Trinadad&Tobago',\n",
       "  39: 'United-States',\n",
       "  40: 'Vietnam',\n",
       "  41: 'Yugoslavia'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_to_orig_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63ba49aebc988b646e1dbf99c75ebbbe79a20618136c52c827b2893e897d8ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
